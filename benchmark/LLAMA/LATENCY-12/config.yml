
version: 1.0
provider:
    name: openfaas
    gateway: http://serverless.siat.ac.cn:31112

functions:
    
    llama-7b-submod-11-latency-12:
        namespace: cdgp
        lang: python3-http
        handler: ./llama-7b-submod-11
        image: k.harbor.siat.ac.cn/openfaas/cdgp-llama-7b-submod-11-latency-12:2.8.2.dasheng
        requests:
            cpu: 1000m
            memory: 8Gi
        limits:
            cpu: 1000m
            memory: 64Gi
        environment:
            read_timeout: "0.83s"
            write_timeout: "0.83s"
            exec_timeout: "0.83s"
            debug: "false"
            infer_device: "cuda"
    
    llama-7b-submod-10-latency-12:
        namespace: cdgp
        lang: python3-http
        handler: ./llama-7b-submod-10
        image: k.harbor.siat.ac.cn/openfaas/cdgp-llama-7b-submod-10-latency-12:2.8.2.dasheng
        requests:
            cpu: 1000m
            memory: 8Gi
        limits:
            cpu: 1000m
            memory: 64Gi
        environment:
            read_timeout: "1.67s"
            write_timeout: "1.67s"
            exec_timeout: "1.67s"
            debug: "false"
            infer_device: "cuda"
    
    llama-7b-submod-9-latency-12:
        namespace: cdgp
        lang: python3-http
        handler: ./llama-7b-submod-9
        image: k.harbor.siat.ac.cn/openfaas/cdgp-llama-7b-submod-9-latency-12:2.8.2.dasheng
        requests:
            cpu: 1000m
            memory: 8Gi
        limits:
            cpu: 1000m
            memory: 64Gi
        environment:
            read_timeout: "2.5s"
            write_timeout: "2.5s"
            exec_timeout: "2.5s"
            debug: "false"
            infer_device: "cuda"
    
    llama-7b-submod-8-latency-12:
        namespace: cdgp
        lang: python3-http
        handler: ./llama-7b-submod-8
        image: k.harbor.siat.ac.cn/openfaas/cdgp-llama-7b-submod-8-latency-12:2.8.2.dasheng
        requests:
            cpu: 1000m
            memory: 8Gi
        limits:
            cpu: 1000m
            memory: 64Gi
        environment:
            read_timeout: "3.33s"
            write_timeout: "3.33s"
            exec_timeout: "3.33s"
            debug: "false"
            infer_device: "cuda"
    
    llama-7b-submod-7-latency-12:
        namespace: cdgp
        lang: python3-http
        handler: ./llama-7b-submod-7
        image: k.harbor.siat.ac.cn/openfaas/cdgp-llama-7b-submod-7-latency-12:2.8.2.dasheng
        requests:
            cpu: 1000m
            memory: 8Gi
        limits:
            cpu: 1000m
            memory: 64Gi
        environment:
            read_timeout: "4.17s"
            write_timeout: "4.17s"
            exec_timeout: "4.17s"
            debug: "false"
            infer_device: "cuda"
    
    llama-7b-submod-6-latency-12:
        namespace: cdgp
        lang: python3-http
        handler: ./llama-7b-submod-6
        image: k.harbor.siat.ac.cn/openfaas/cdgp-llama-7b-submod-6-latency-12:2.8.2.dasheng
        requests:
            cpu: 1000m
            memory: 8Gi
        limits:
            cpu: 1000m
            memory: 64Gi
        environment:
            read_timeout: "5.0s"
            write_timeout: "5.0s"
            exec_timeout: "5.0s"
            debug: "false"
            infer_device: "cuda"
    
    llama-7b-submod-5-latency-12:
        namespace: cdgp
        lang: python3-http
        handler: ./llama-7b-submod-5
        image: k.harbor.siat.ac.cn/openfaas/cdgp-llama-7b-submod-5-latency-12:2.8.2.dasheng
        requests:
            cpu: 1000m
            memory: 8Gi
        limits:
            cpu: 1000m
            memory: 64Gi
        environment:
            read_timeout: "5.83s"
            write_timeout: "5.83s"
            exec_timeout: "5.83s"
            debug: "false"
            infer_device: "cuda"
    
    llama-7b-submod-4-latency-12:
        namespace: cdgp
        lang: python3-http
        handler: ./llama-7b-submod-4
        image: k.harbor.siat.ac.cn/openfaas/cdgp-llama-7b-submod-4-latency-12:2.8.2.dasheng
        requests:
            cpu: 1000m
            memory: 8Gi
        limits:
            cpu: 1000m
            memory: 64Gi
        environment:
            read_timeout: "6.67s"
            write_timeout: "6.67s"
            exec_timeout: "6.67s"
            debug: "false"
            infer_device: "cuda"
    
    llama-7b-submod-3-latency-12:
        namespace: cdgp
        lang: python3-http
        handler: ./llama-7b-submod-3
        image: k.harbor.siat.ac.cn/openfaas/cdgp-llama-7b-submod-3-latency-12:2.8.2.dasheng
        requests:
            cpu: 1000m
            memory: 8Gi
        limits:
            cpu: 1000m
            memory: 64Gi
        environment:
            read_timeout: "7.5s"
            write_timeout: "7.5s"
            exec_timeout: "7.5s"
            debug: "false"
            infer_device: "cuda"
    
    llama-7b-submod-2-latency-12:
        namespace: cdgp
        lang: python3-http
        handler: ./llama-7b-submod-2
        image: k.harbor.siat.ac.cn/openfaas/cdgp-llama-7b-submod-2-latency-12:2.8.2.dasheng
        requests:
            cpu: 1000m
            memory: 8Gi
        limits:
            cpu: 1000m
            memory: 64Gi
        environment:
            read_timeout: "8.33s"
            write_timeout: "8.33s"
            exec_timeout: "8.33s"
            debug: "false"
            infer_device: "cuda"
    
    llama-7b-submod-1-latency-12:
        namespace: cdgp
        lang: python3-http
        handler: ./llama-7b-submod-1
        image: k.harbor.siat.ac.cn/openfaas/cdgp-llama-7b-submod-1-latency-12:2.8.2.dasheng
        requests:
            cpu: 1000m
            memory: 8Gi
        limits:
            cpu: 1000m
            memory: 64Gi
        environment:
            read_timeout: "9.17s"
            write_timeout: "9.17s"
            exec_timeout: "9.17s"
            debug: "false"
            infer_device: "cuda"
    
    llama-7b-submod-0-latency-12:
        namespace: cdgp
        lang: python3-http
        handler: ./llama-7b-submod-0
        image: k.harbor.siat.ac.cn/openfaas/cdgp-llama-7b-submod-0-latency-12:2.8.2.dasheng
        requests:
            cpu: 1000m
            memory: 8Gi
        limits:
            cpu: 1000m
            memory: 64Gi
        environment:
            read_timeout: "10.0s"
            write_timeout: "10.0s"
            exec_timeout: "10.0s"
            debug: "false"
            infer_device: "cuda"
    